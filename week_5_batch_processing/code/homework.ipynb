{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26826969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/01 16:26:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType, TimestampType, LongType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b51dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = 'data/raw/fhvhv/2021/02/'\n",
    "pq_path = 'data/pq/fhvhv/2021/02/'\n",
    "pq_path_temp = pq_path + 'tmp/'\n",
    "file_name = 'fhvhv_tripdata_2021-02.csv'\n",
    "url_base = 'https://nyc-tlc.s3.amazonaws.com/trip+data/'\n",
    "url = url_base + file_name\n",
    "raw_path_file = raw_path + file_name\n",
    "pq_path_file = pq_path + file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d50cb",
   "metadata": {},
   "source": [
    "## QUESTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3ed8c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9682ec",
   "metadata": {},
   "source": [
    "## QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7940f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p raw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6d6bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-28 21:40:15--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.136.113\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.136.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘data/raw/fhvhv/2021/02/fhvhv_tripdata_2021-02.csv’\n",
      "\n",
      "data/raw/fhvhv/2021 100%[===================>] 699.83M  33.5MB/s    in 21s     \n",
      "\n",
      "2022-02-28 21:40:37 (32.6 MB/s) - ‘data/raw/fhvhv/2021/02/fhvhv_tripdata_2021-02.csv’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget {url} -O {raw_path_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8b012f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag\r",
      "\r\n",
      "HV0003,B02764,2021-02-01 00:10:40,2021-02-01 00:21:09,35,39,\r",
      "\r\n",
      "HV0003,B02764,2021-02-01 00:27:23,2021-02-01 00:44:01,39,35,\r",
      "\r\n",
      "HV0005,B02510,2021-02-01 00:28:38,2021-02-01 00:38:27,39,91,\r",
      "\r\n",
      "HV0005,B02510,2021-02-01 00:43:37,2021-02-01 01:23:20,91,228,\r",
      "\r\n",
      "HV0003,B02872,2021-02-01 00:08:42,2021-02-01 00:17:57,126,250,\r",
      "\r\n",
      "HV0003,B02872,2021-02-01 00:26:02,2021-02-01 00:42:51,208,243,\r",
      "\r\n",
      "HV0003,B02872,2021-02-01 00:45:50,2021-02-01 01:02:50,243,220,\r",
      "\r\n",
      "HV0003,B02764,2021-02-01 00:06:42,2021-02-01 00:31:50,49,37,\r",
      "\r\n",
      "HV0003,B02764,2021-02-01 00:34:34,2021-02-01 00:58:13,37,76,\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head {raw_path_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d70a969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType(), True),\n",
    "    StructField(\"dispatching_base_num\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"SR_Flag\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bffe8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".options(header=True) \\\n",
    ".schema(schema) \\\n",
    ".csv(raw_path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "146655ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:45:50|2021-02-01 01:02:50|         243|         220|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:06:42|2021-02-01 00:31:50|          49|          37|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:34:34|2021-02-01 00:58:13|          37|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:03:43|2021-02-01 00:39:37|          80|         241|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:55:36|2021-02-01 01:08:39|         174|          51|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:06:13|2021-02-01 00:33:45|         235|         129|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:42:24|2021-02-01 01:11:31|         129|         169|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:07:05|2021-02-01 00:20:53|         226|          82|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:28:56|2021-02-01 00:33:59|          82|         129|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:44:53|2021-02-01 01:07:54|           7|          79|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:17:55|2021-02-01 00:34:41|           4|         170|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:38:14|2021-02-01 00:59:20|         164|          42|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:08:04|2021-02-01 00:24:41|         237|           4|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:30:44|2021-02-01 00:41:26|         107|          45|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a768f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(24).write.parquet(pq_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6f455f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 212936\r\n",
      "-rw-r--r-- 1 marcos marcos       0 Mar  1 11:33 _SUCCESS\r\n",
      "-rw-r--r-- 1 marcos marcos 9080331 Mar  1 11:33 part-00000-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9080702 Mar  1 11:33 part-00001-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9082175 Mar  1 11:33 part-00002-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9082899 Mar  1 11:33 part-00003-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9082402 Mar  1 11:33 part-00004-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9085799 Mar  1 11:33 part-00005-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9086884 Mar  1 11:33 part-00006-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9083085 Mar  1 11:33 part-00007-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9086649 Mar  1 11:33 part-00008-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9082765 Mar  1 11:33 part-00009-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9082717 Mar  1 11:33 part-00010-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9084380 Mar  1 11:33 part-00011-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9082358 Mar  1 11:33 part-00012-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9084167 Mar  1 11:33 part-00013-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9083700 Mar  1 11:33 part-00014-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9079965 Mar  1 11:33 part-00015-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9083294 Mar  1 11:33 part-00016-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9085434 Mar  1 11:33 part-00017-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9079801 Mar  1 11:33 part-00018-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9082749 Mar  1 11:33 part-00019-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9083602 Mar  1 11:33 part-00020-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9082958 Mar  1 11:33 part-00021-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9085145 Mar  1 11:33 part-00022-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 marcos marcos 9084529 Mar  1 11:33 part-00023-706541da-60e7-49cb-a186-af6240e3268a-c000.snappy.parquet\r\n",
      "drwxr-xr-x 2 marcos marcos    4096 Mar  1 11:35 tmp\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {pq_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aeac10",
   "metadata": {},
   "source": [
    "## QUESTION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2fea7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(pq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d28861ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('trip_duration', df.dropoff_datetime.cast(\"long\") - df.pickup_datetime.cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "daec1474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 75:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+-------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|pickup_date|trip_duration|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+-------------+\n",
      "|           HV0003|              B02887|2021-02-06 01:18:35|2021-02-06 01:40:34|         163|         235|   null| 2021-02-06|         1319|\n",
      "|           HV0005|              B02510|2021-02-05 07:13:06|2021-02-05 07:31:56|         225|         181|   null| 2021-02-05|         1130|\n",
      "|           HV0003|              B02869|2021-02-04 16:56:52|2021-02-04 17:21:36|         260|          95|   null| 2021-02-04|         1484|\n",
      "|           HV0003|              B02871|2021-02-03 18:34:17|2021-02-03 18:57:12|         235|          60|   null| 2021-02-03|         1375|\n",
      "|           HV0003|              B02869|2021-02-04 07:25:09|2021-02-04 07:30:34|          55|          55|   null| 2021-02-04|          325|\n",
      "|           HV0003|              B02836|2021-02-04 23:15:27|2021-02-04 23:34:29|          74|          81|   null| 2021-02-04|         1142|\n",
      "|           HV0003|              B02882|2021-02-05 07:45:49|2021-02-05 08:05:03|         239|         231|   null| 2021-02-05|         1154|\n",
      "|           HV0003|              B02887|2021-02-03 17:33:59|2021-02-03 17:47:14|          95|         196|   null| 2021-02-03|          795|\n",
      "|           HV0003|              B02877|2021-02-03 09:33:06|2021-02-03 09:45:59|         241|         265|   null| 2021-02-03|          773|\n",
      "|           HV0003|              B02872|2021-02-02 12:58:46|2021-02-02 13:06:20|          90|         137|   null| 2021-02-02|          454|\n",
      "|           HV0003|              B02764|2021-02-05 22:25:20|2021-02-05 22:28:27|           7|           7|   null| 2021-02-05|          187|\n",
      "|           HV0003|              B02882|2021-02-04 19:57:44|2021-02-04 20:05:24|         231|         246|   null| 2021-02-04|          460|\n",
      "|           HV0003|              B02764|2021-02-02 08:43:57|2021-02-02 08:53:46|         108|          29|   null| 2021-02-02|          589|\n",
      "|           HV0003|              B02866|2021-02-04 05:51:50|2021-02-04 06:21:25|          11|         112|   null| 2021-02-04|         1775|\n",
      "|           HV0003|              B02882|2021-02-05 16:13:02|2021-02-05 16:27:48|         229|         234|   null| 2021-02-05|          886|\n",
      "|           HV0005|              B02510|2021-02-04 17:15:28|2021-02-04 17:24:46|          10|         218|   null| 2021-02-04|          558|\n",
      "|           HV0003|              B02617|2021-02-04 09:59:58|2021-02-04 10:29:13|          49|         232|   null| 2021-02-04|         1755|\n",
      "|           HV0005|              B02510|2021-02-05 23:34:27|2021-02-05 23:51:05|         125|          17|   null| 2021-02-05|          998|\n",
      "|           HV0003|              B02875|2021-02-05 20:52:58|2021-02-05 21:19:59|          47|          68|   null| 2021-02-05|         1621|\n",
      "|           HV0003|              B02875|2021-02-04 18:11:00|2021-02-04 18:37:24|          41|         223|   null| 2021-02-04|         1584|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4db1c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(pq_path_temp, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "13bd8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(pq_path_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bfa63c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367170"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.pickup_date == '2021-02-15').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f0601",
   "metadata": {},
   "source": [
    "## QUESTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "462259d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 100:==========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "date_of_longest_trip = df.orderBy(df.trip_duration.desc()).take(1)[0].pickup_date\n",
    "print(date_of_longest_trip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be58913",
   "metadata": {},
   "source": [
    "Note: orderBy(df._count.desc()) will not work. Error dataframe object has no attribute _count is raised. Use F.col instead\n",
    "So aggregators are accessed through F.col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375bec7",
   "metadata": {},
   "source": [
    "## QUESTION 5 (check number of stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e3b55f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 158:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num| _count|\n",
      "+--------------------+-------+\n",
      "|              B02510|3233664|\n",
      "|              B02764| 965568|\n",
      "|              B02872| 882689|\n",
      "|              B02875| 685390|\n",
      "|              B02765| 559768|\n",
      "|              B02869| 429720|\n",
      "|              B02887| 322331|\n",
      "|              B02871| 312364|\n",
      "|              B02864| 311603|\n",
      "|              B02866| 311089|\n",
      "|              B02878| 305185|\n",
      "|              B02682| 303255|\n",
      "|              B02617| 274510|\n",
      "|              B02883| 251617|\n",
      "|              B02884| 244963|\n",
      "|              B02882| 232173|\n",
      "|              B02876| 215693|\n",
      "|              B02879| 210137|\n",
      "|              B02867| 200530|\n",
      "|              B02877| 198938|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .groupBy(\"dispatching_base_num\") \\\n",
    "    .agg(F.count(\"dispatching_base_num\") \\\n",
    "    .alias(\"_count\")) \\ \n",
    "    .orderBy(F.col(\"_count\").desc()) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67311a69",
   "metadata": {},
   "source": [
    "## QUESTION 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3d34cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289a794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(pq_path_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11f793",
   "metadata": {},
   "source": [
    "Notes: \n",
    "Need to use alias in the df name for df_zones in order to distinguish later on when using aggregations\n",
    "Note: If I do not use alias and just df_zones.LocationID in both joins instead of the F.col with the alias, the Java explodes with memory problems.\n",
    "\n",
    "Join needs a column object in ==, so use F.col\n",
    "\n",
    "Need to use first \"aggregator\" to include that column in aggregations because after group by, only the columns that are used in the group by are kept, in addition to the aggregators\n",
    "\n",
    "Note: withColumnRenamed needs the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b906e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip = df \\\n",
    "    .join(df_zones.alias(\"df_zones_pu\"), df.PULocationID == F.col(\"df_zones_pu.LocationID\")) \\\n",
    "    .join(df_zones.alias(\"df_zones_do\"), df.DOLocationID == F.col(\"df_zones_do.LocationID\")) \\\n",
    "    .groupBy('PULocationID','DOLocationID') \\\n",
    "    .agg(F.count('DOLocationID').alias('_count'), F.first(\"df_zones_pu.Zone\"), F.first(\"df_zones_do.Zone\")) \\\n",
    "    .withColumnRenamed('first(df_zones_pu.Zone)', 'pickup_zone') \\\n",
    "    .withColumnRenamed('first(df_zones_do.Zone)', 'dropoff_zone') \\\n",
    "    .orderBy(F.col('_count').desc()) \\\n",
    "    .first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa8ed9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'East New York / East New York'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{trip[\"pickup_zone\"]} / {trip[\"dropoff_zone\"]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a56a8",
   "metadata": {},
   "source": [
    "Note: Also I can join after grouping and aggregation, but use 'order by' after joins (otherwise the order is lost). This performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "f745cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip = df \\\n",
    "    .groupBy('PULocationID','DOLocationID') \\\n",
    "    .agg(F.count('DOLocationID').alias('_count')) \\\n",
    "    .join(df_zones.withColumnRenamed('Zone', 'pickup_zone').alias(\"df_zones_pu\"), df.PULocationID == F.col(\"df_zones_pu.LocationID\")) \\\n",
    "    .join(df_zones.withColumnRenamed('Zone', 'dropoff_zone').alias(\"df_zones_do\"), df.DOLocationID == F.col(\"df_zones_do.LocationID\")) \\\n",
    "    .orderBy(F.col('_count').desc()) \\\n",
    "    .first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "5051e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'East New York / East New York'"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{trip[\"pickup_zone\"]} / {trip[\"dropoff_zone\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29351a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
