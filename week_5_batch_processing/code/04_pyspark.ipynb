{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a90a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, DateType, StructField, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a3667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/24 09:55:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d7ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-22 22:07:42--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-01.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.141.233\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.141.233|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 752335705 (717M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-01.csv.1’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 717.48M  35.7MB/s    in 21s     \n",
      "\n",
      "2022-02-22 22:08:04 (33.8 MB/s) - ‘fhvhv_tripdata_2021-01.csv.1’ saved [752335705/752335705]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b08259",
   "metadata": {},
   "source": [
    "Note: inferSchema=True, header=True will inferr types but dates will still be string. \n",
    "Passing the schema when reading the csv works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "793559e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType(), True),\n",
    "    StructField(\"dispatching_base_num\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"SR_Flag\", StringType(), True),\n",
    "])\n",
    "df = spark.read \\\n",
    "    .options(header=True) \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "954cc9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,TimestampType,true),StructField(dropoff_datetime,TimestampType,true),StructField(PULocationID,IntegerType,true),StructField(DOLocationID,IntegerType,true),StructField(SR_Flag,StringType,true)))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b7324",
   "metadata": {},
   "source": [
    "We can create spark dataframe from pandas dataframe. However, some conversion is done. Let's check final schema after conversion. It is better to set the schema as previously done, using integer (4 bytes) avoiding the automatic float64 or long (8 bytes in both cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb38e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8688351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9eaac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f311f9cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2133/3253771371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pandas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_pandas' is not defined"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50eacf",
   "metadata": {},
   "source": [
    "Note that this conversion takes too long. It is better to read csv with spark and set the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c92a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3bfab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/01', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69b2cb",
   "metadata": {},
   "source": [
    "Writting to parket format will use several executors in the cluster for parallel processing the partitions. repartition is a logic command, nothing is processed until a processing command is performed, such as write.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc68717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4a320bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b823a82",
   "metadata": {},
   "source": [
    "Select and filter, joins, group by... are transformations, not executed, these are lazy. Show, take, head, write... are actions, so previous related transformations and action are executed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f42ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e993b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "858634ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f35b90",
   "metadata": {},
   "source": [
    "Note: select returns dataframe. df.xxxx returnscolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5889a5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+-------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|base_id|\n",
      "+-----------+------------+------------+------------+-------+\n",
      "| 2021-01-07|  2021-01-07|         142|         230|  e/9ce|\n",
      "| 2021-01-01|  2021-01-01|         133|          91|  e/9ce|\n",
      "| 2021-01-01|  2021-01-01|         147|         159|  e/acc|\n",
      "| 2021-01-06|  2021-01-06|          79|         164|  e/b35|\n",
      "| 2021-01-04|  2021-01-04|         174|          18|  s/b44|\n",
      "| 2021-01-04|  2021-01-04|         201|         180|  e/b3b|\n",
      "| 2021-01-04|  2021-01-04|         230|         142|  e/9ce|\n",
      "| 2021-01-03|  2021-01-03|         132|          72|  e/b38|\n",
      "| 2021-01-01|  2021-01-01|         188|          61|  s/af0|\n",
      "| 2021-01-04|  2021-01-04|          97|         189|  e/9ce|\n",
      "| 2021-01-01|  2021-01-01|         174|         235|  e/acc|\n",
      "| 2021-01-05|  2021-01-05|          35|          76|  e/b37|\n",
      "| 2021-01-06|  2021-01-06|          35|          39|  e/9ce|\n",
      "| 2021-01-04|  2021-01-04|         231|          13|  e/b42|\n",
      "| 2021-01-02|  2021-01-02|          87|         127|  e/a39|\n",
      "| 2021-01-02|  2021-01-02|          17|          89|  e/9ce|\n",
      "| 2021-01-02|  2021-01-02|          11|          14|  e/b35|\n",
      "| 2021-01-01|  2021-01-01|          21|          26|  e/9ce|\n",
      "| 2021-01-04|  2021-01-04|          83|         260|  e/b35|\n",
      "| 2021-01-01|  2021-01-01|         189|          52|  e/9ce|\n",
      "+-----------+------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID', 'base_id') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc31fdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 13), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 30, 35), PULocationID=147, DOLocationID=159, SR_Flag=None),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 11, 43, 12), dropoff_datetime=datetime.datetime(2021, 1, 6, 11, 55, 7), PULocationID=79, DOLocationID=164, SR_Flag=None),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 4, 15, 35, 32), dropoff_datetime=datetime.datetime(2021, 1, 4, 15, 52, 2), PULocationID=174, DOLocationID=18, SR_Flag=None),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 4, 13, 42, 15), dropoff_datetime=datetime.datetime(2021, 1, 4, 14, 4, 57), PULocationID=201, DOLocationID=180, SR_Flag=None),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 3, 18, 42, 3), dropoff_datetime=datetime.datetime(2021, 1, 3, 19, 12, 22), PULocationID=132, DOLocationID=72, SR_Flag=None)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'SR_Flag') \\\n",
    ".filter(df.hvfhs_license_num == 'HV0003') \\\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ba760",
   "metadata": {},
   "source": [
    "Note: Take returns a list, not a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7caf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
