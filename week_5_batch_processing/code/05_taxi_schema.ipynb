{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bc52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "import logging, traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "382ddbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "540033b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_schema = types.StructType([\n",
    "    types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "    types.StructField(\"lpep_pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"lpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"passenger_count\", types.IntegerType(), True),\n",
    "    types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "    types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"extra\", types.DoubleType(), True),\n",
    "    types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "    types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"ehail_fee\", types.DoubleType(), True),\n",
    "    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "    types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"payment_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"trip_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n",
    "])\n",
    "\n",
    "yellow_schema = types.StructType([\n",
    "    types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "    types.StructField(\"tpep_pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"tpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"passenger_count\", types.IntegerType(), True),\n",
    "    types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n",
    "    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"payment_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"extra\", types.DoubleType(), True),\n",
    "    types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "    types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "    types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n",
    "])\n",
    "schema = { 'green': green_schema, 'yellow': yellow_schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a77e1065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 14, in <module>\n",
      "    df.repartition(4).write.parquet(output_path)\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 939, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: path file:/home/marcos/code/data/pq/yellow/2020/01 already exists.;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/1\n",
      "Data for yellow/2020/1 not found or already processed. Continue with next\n",
      "Processing data for yellow/2020/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2020/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Traceback (most recent call last):                                   \n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 14, in <module>\n",
      "    df.repartition(4).write.parquet(output_path)\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 939, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: path file:/home/marcos/code/data/pq/green/2020/01 already exists.;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/1\n",
      "Data for green/2020/1 not found or already processed. Continue with next\n",
      "Processing data for green/2020/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2020/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Traceback (most recent call last):                                   \n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 14, in <module>\n",
      "    df.repartition(4).write.parquet(output_path)\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 939, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: path file:/home/marcos/code/data/pq/yellow/2021/01 already exists.;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2021/1\n",
      "Data for yellow/2021/1 not found or already processed. Continue with next\n",
      "Processing data for yellow/2021/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2021/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2021/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2021/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2021/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2021/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Traceback (most recent call last):                                   \n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 9, in <module>\n",
      "    df = spark.read \\\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 538, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/marcos/code/data/raw/yellow/2021/09;\n",
      "\n",
      "ERROR:root:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 9, in <module>\n",
      "    df = spark.read \\\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 538, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/marcos/code/data/raw/yellow/2021/10;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for yellow/2021/8\n",
      "Processing data for yellow/2021/9\n",
      "Data for yellow/2021/9 not found or already processed. Continue with next\n",
      "Processing data for yellow/2021/10\n",
      "Data for yellow/2021/10 not found or already processed. Continue with next\n",
      "Processing data for yellow/2021/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 9, in <module>\n",
      "    df = spark.read \\\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 538, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/marcos/code/data/raw/yellow/2021/11;\n",
      "\n",
      "ERROR:root:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 9, in <module>\n",
      "    df = spark.read \\\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 538, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/marcos/code/data/raw/yellow/2021/12;\n",
      "\n",
      "ERROR:root:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 14, in <module>\n",
      "    df.repartition(4).write.parquet(output_path)\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 939, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: path file:/home/marcos/code/data/pq/green/2021/01 already exists.;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for yellow/2021/11 not found or already processed. Continue with next\n",
      "Processing data for yellow/2021/12\n",
      "Data for yellow/2021/12 not found or already processed. Continue with next\n",
      "Processing data for green/2021/1\n",
      "Data for green/2021/1 not found or already processed. Continue with next\n",
      "Processing data for green/2021/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2021/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2021/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2021/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2021/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2021/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Traceback (most recent call last):                                   \n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 9, in <module>\n",
      "    df = spark.read \\\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 538, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/marcos/code/data/raw/green/2021/09;\n",
      "\n",
      "ERROR:root:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 9, in <module>\n",
      "    df = spark.read \\\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 538, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/marcos/code/data/raw/green/2021/10;\n",
      "\n",
      "ERROR:root:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 9, in <module>\n",
      "    df = spark.read \\\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 538, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/marcos/code/data/raw/green/2021/11;\n",
      "\n",
      "ERROR:root:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_6984/3421193845.py\", line 9, in <module>\n",
      "    df = spark.read \\\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 538, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marcos/bin/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/marcos/code/data/raw/green/2021/12;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for green/2021/8\n",
      "Processing data for green/2021/9\n",
      "Data for green/2021/9 not found or already processed. Continue with next\n",
      "Processing data for green/2021/10\n",
      "Data for green/2021/10 not found or already processed. Continue with next\n",
      "Processing data for green/2021/11\n",
      "Data for green/2021/11 not found or already processed. Continue with next\n",
      "Processing data for green/2021/12\n",
      "Data for green/2021/12 not found or already processed. Continue with next\n"
     ]
    }
   ],
   "source": [
    "for year in [2020, 2021]:\n",
    "    for type in ['yellow', 'green']:\n",
    "        for month in range(1,13):\n",
    "            try:\n",
    "                print(f'Processing data for {type}/{year}/{month}')\n",
    "                input_path = f'data/raw/{type}/{year}/{month:02d}/'\n",
    "                output_path = f'data/pq/{type}/{year}/{month:02d}/'\n",
    "                \n",
    "                df = spark.read \\\n",
    "                .options(header=True) \\\n",
    "                .schema(schema[type]) \\\n",
    "                .csv(input_path)\n",
    "                \n",
    "                df.repartition(4).write.parquet(output_path, mode='overwrite')\n",
    "                \n",
    "            except Exception as ex:\n",
    "                logging.error(traceback.format_exc())\n",
    "                print(f'Data for {type}/{year}/{month} not found or already processed. Continue with next')\n",
    "                continue\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc99ab",
   "metadata": {},
   "source": [
    "The other way to infer the schema (apart from pandas) for the csv files, is to set the inferSchema option to true while reading the files in Spark.\n",
    "However, the datetime is detected as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789d67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
